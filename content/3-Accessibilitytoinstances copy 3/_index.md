---
title : "Extend the LLM inference beyond AWS through Remote Inference"
date : "2025-02-21"
weight : 6
chapter : false
pre : " <b> 6. </b> "
---

In this lab, you will learn how to integrate inference APIs that are compatible with OpenAIâ€™s API, extending LLM inference beyond AWS. You will explore the necessary modifications to the EKS deployment and the OPEA pipeline to enable remote inference. This lab leverages pre-installed inference APIs in Denvr Dataworks, powered by Intel Gaudi2 AI Accelerator instances. Additionally, you will have the flexibility to migrate these APIs to managed services, demonstrating how OPEA inference services (such as vLLM and TGI) can be deployed across multiple cloud providers to create a seamless and scalable inference pipeline.

**Learning Objectives**
+ Understand the modifications required in OPEA for integrating managed inference services.
+ Explore Intel Gaudi2 AI Accelerators and their role in optimizing inference performance.
+ Learn how OPEA-powered inference services operate in Denvr Dataworks Cloud.
+ Evaluate and compare the response quality, accuracy, and performance of different models across various environments.