---
title : "OpenSearch integration"
date : "2025-02-21"
weight : 1
chapter : false
pre : " <b> 5.1. </b> "
---
**Using OpenSearch as the Vector Database for OPEA**

OpenSearch is a powerful search, analytics, and vector database solution. Developed under the OpenSearch Software Foundation, a member of the Linux Foundation, it provides open governance for the OpenSearch project on GitHub. Amazon OpenSearch Service offers a managed solution that simplifies deployment, scaling, and operations in the AWS cloud. Whether self-managing OpenSearch on-premises or in the cloud, or using Amazon OpenSearch Service, OpenSearch delivers high-performance vector search capabilities to support fast, accurate, and scalable lexical and semantic searchâ€”enabling more relevant results for websites and Generative AI applications.

At its core, OpenSearch is a search engine. Search engines play a vital role in our daily lives, helping us find information, products, travel destinations, and restaurants. When using a search engine, users provide a text query, sometimes refined with additional filters, and the engine returns a list of results relevant to the user's search intent.

As a specialized database, OpenSearch works alongside traditional data stores (such as relational databases) to deliver low-latency, high-throughput search across large volumes of unstructured text, structured metadata fields, and vector embeddings. The foundation of OpenSearch is the index, similar to a database table, which stores documents in JSON format. Each document contains fields (JSON keys) and corresponding values. Users can query documents using text, numeric ranges, dates, geographic data, and more. When searching unstructured text, OpenSearch ranks documents based on a relevance score, favoring rare words highly concentrated within the document.

Recent advancements in Natural Language Processing (NLP) and Large Language Models (LLMs) have transformed how we search for and retrieve information. LLMs encode language into high-dimensional vector spaces, enabling searches that match meaning rather than exact words. They also generate realistic, human-like responses to natural language queries.

This shift allows users to interact with information retrieval systems through conversational AI, multimodal search (text, images, audio, video), and advanced knowledge discovery.

OpenSearch plays two critical roles in AI-powered applications:

1. Semantic Search with Vector Embeddings
OpenSearch can store and retrieve vector embeddings generated by embedding models. It performs nearest-neighbor searches to find documents whose vectors are closest to the query's embedding, improving search accuracy by matching intent rather than keywords.

2. Retrieval-Augmented Generation (RAG) for Generative AI
OpenSearch can function as a knowledge base, retrieving relevant information to enhance LLM-generated responses. By integrating OpenSearch into OPEA, we improve the accuracy and reliability of AI-generated outputs by grounding responses in factual, real-world data.

![VPC](/images/4.s3/image080.png)

Thanks to the interchangeability offered by OPEA, most of the components from the default [ChatQnA example](https://github.com/opea-project/GenAIExamples/tree/main/ChatQnA)(Module 1). Of course, for this module, you'll need to deploy OpenSearch. Additionally, you'll use a retriever and a data prep microservice designed to work with OpenSearch's query and indexing APIs. You can find these components in the [OPEA GitHub project's components directory](https://github.com/opea-project/GenAIComps/tree/main/comps) .

**Deploying ChatQnA using OpenSearch as vector database**
For this lab, we've created a changeset that you can deploy that contains a full, parallel deployment of the ChatQnA example, in the same Kubernetes cluster you've been using. We've wrapped all of the changes you need in an AWS CloudFormation changeset that you deploy with the command below.

{{% notice info %}}
We've used a Kubernetes namespace, opensearch, to separate out the pods and services pertaining to the OpenSearch deployment. When you use kubectl and other Kubernetes commands in the below examples, be sure to qualify the command with -n opensearch.
{{% /notice %}}

Return to your Cloud Shell. If the shell has terminated, click in the window to open a new command line, or use the icon at the top of the AWS console to start a new Cloud Shell. Use the following command to deploy the OpenSearch change set.

![VPC](/images/4.s3/image081.png)

OpenSearch will take a few minutes to deploy. To check the status, you can use kubectl to monitor the state of the OpenSearch pods. You can use the command

![VPC](/images/4.s3/image082.png)

to get output like this

![VPC](/images/4.s3/image083.png)

Only continue when you see the opensearch-cluster-master-0 pod in the Running state.

**Changes to ChatQnA for OpenSearch**

Much of the deployment and components for ChatQnA are identical when you use OpenSearch as a back end, compared with other vector database back ends. OPEA contains components for preparing and sending data to the vector database (chatqna-data-prep) and for retrieving data from the vector database (chatqna-retriever-usvc). You can find the OpenSearch implementations  for these components on GitHub.

Schematically, you are now talking to OpenSearch via the OpenSearch-specific components:

![VPC](/images/4.s3/image084.png)

**Understanding OpenSearchâ€™s Distributed System**
OpenSearch is a distributed database that operates on a cluster of nodes, each of which can take on different roles to optimize performance and scalability. A node can serve multiple roles simultaneously, enabling it to perform various functions within the cluster. Some of the key node roles include:

+ **Data Nodes**: These nodes handle data storage and processing. They manage indexing and search requests, making them the backbone of OpenSearchâ€™s distributed architecture.

+ **Cluster Manager Nodes**: Previously known as master nodes, these are responsible for orchestrating the cluster. They maintain and distribute the cluster state, monitor node activity, and ensure overall system stability. While a node can have both data and master roles, separating these roles onto dedicated hardware is recommended for better performance and reliability.

+ **ML Nodes**: These nodes support machine learning capabilities by running the ml-commons plugin. They can host and execute machine learning models, including large language models, enabling advanced data processing and analytics.
By leveraging heterogeneous hardware, you can strategically assign roles to nodes based on their intended function, ensuring efficient resource utilization.

OpenSearch interacts with users through a RESTful API, with requests typically routed via a load balancer to distribute the workload across available data nodes. When a data node receives a request, it acts as a coordinator, delegating sub-requests to relevant data nodes that store the required data. Each of these nodes processes its portion of the request and sends results back to the coordinating node, which aggregates them into a final response before returning it to the client.

**Understanding OpenSearchâ€™s Data Handling**
At the core of OpenSearch's data structure is the index, which serves as a logical collection of documents containing structured data. To manage data distribution efficiently, each index is divided into shardsâ€”independent storage units that allow parallel processing.

Each shard is essentially an instance of Apache Lucene, a powerful Java-based search library that reads and writes search indices. When an index is created, you define the number of primary shards, which determines how the data will be initially partitioned. When a document is indexed, OpenSearch assigns it to a primary shard using a randomized distribution strategy, ensuring balanced storage and retrieval performance across the cluster.

![VPC](/images/4.s3/image085.png)

OpenSearch distributes the shards across the available data nodes in the cluster. You can also set one or more replicas for the index. Each replica is a complete copy of the set of primary shards. For example, if you have 5 primary shards and one replica, you have 10 shards in total. Your decisions about the primary shard count, the replica count, and the number of data nodes is of critical importance to the sucess of your cluster in handling your workload.

When sizing your cluster, here are some guidelines to help you achieve success.

{{% notice info %}}
Amazon OpenSearch Service's documentation also includes best practices on a number of topics, including sizing OpenSearch Service domains . While these best practices are service-specific, the documentation details first principles that will help you size your self-managed domain as well.
{{% /notice %}}

### Calculating Storage Requirements for Metadata and Vectors in OpenSearch
To properly size your OpenSearch cluster, start by estimating the storage needed for both vector data and metadata.

**Vector Storage Calculation**
Use the following formula to determine the storage required for vectors:

BytesÂ perÂ dimension
Ã—
NumberÂ ofÂ dimensions
Ã—
NumberÂ ofÂ vectors
Ã—
(
1
+
NumberÂ ofÂ replicas
)
BytesÂ perÂ dimensionÃ—NumberÂ ofÂ dimensionsÃ—NumberÂ ofÂ vectorsÃ—(1+NumberÂ ofÂ replicas)
For example, if your vectors use floating-point numbers (default, 4 bytes per dimension), are 768-dimensional, and you have 1 million vectors with 1 replica, your vector storage requirement is:

4
Ã—
768
Ã—
1
,
000
,
000
Ã—
2
=
6
ðº
ðµ
4Ã—768Ã—1,000,000Ã—2=6GB
**Metadata Storage Calculation**
For metadata, use the formula:

(
1
+
NumberÂ ofÂ replicas
)
Ã—
SizeÂ ofÂ sourceÂ dataÂ (inÂ bytes)
Ã—
1.10
(1+NumberÂ ofÂ replicas)Ã—SizeÂ ofÂ sourceÂ dataÂ (inÂ bytes)Ã—1.10
Here, 1.10 is an inflation factor accounting for the difference between raw source data size and indexed storage size. If your metadata size is 100 GB with one replica, the total metadata storage needed is:

2
Ã—
100
ðº
ðµ
Ã—
1.10
=
220
ðº
ðµ
2Ã—100GBÃ—1.10=220GB
**Total Storage Requirement**
Sum the vector storage and metadata storage to get the total storage needed for your OpenSearch cluster.

### Verify the OpenSearch Deployment
To verify the deployment, you will use Kubernetes port forwarding to call the various microservices. You can verify the OpenSearch deployment is working by executing a GET request against the base URL. OpenSearch listens on port 9200. Use the following command to map port 9200 to your local port 9200. (If your Cloud Shell terminal has terminated, click in the window or open a new terminal from the AWS console.)

![VPC](/images/image086.png)

{{% notice info %}}
In this section of the guide, you will use port forwarding for a number of services. The port forwarding takes a few seconds to start, be sure to wait for a confirmation line looking like this: Forwarding from 127.0.0.1:9200 -> 9200.
{{% /notice %}}

Now you can query OpenSearch directly on localhost:9200. OpenSearch supports encrypted communication via Transport Layer Security (TLS), and ships with a demo certificate that is not signed by an authority. For demo purposes, you'll use the --insecure option for curl. OpenSearch's fine-grained access control supports an internal user/password database for basic HTTP authentication. It can also integrate with Security Assertion Markup Language (SAML) identity providers for login to OpenSearch Dashboards (OpenSearch's user interface). You'll provide HTTP authentication with your curl request.

A query to / simply returns cluster health and information.

![VPC](/images/5.fwd/image087.png)

You should receive output like this:

![VPC](/images/5.fwd/image088.png)

Congratulations! You've created an OpenSearch-backed deployment of OPEA's ChatQnA example!

**Working with OPEA ChatQnA and OpenSearch**

You can work with the individual microservices in the OpenSearch deployment in the same way you did with the Redis deployment. In this section you'll add a sample document to OpenSearch via the chatqna-data-prep service, generate an embedding, and query OpenSearch with that embedding via the chatqna-retriever-usvc. As you work through this section of the guide, you'll see that choosing OpenSearch as your vector DB is somewhat transparent at the data prep and retriever levels. The OpenSearch service provides a facade that enables other OPEA components to "just use it".

**Upload a document to OpenSearch**
Download the sample pdf document with the below command

curl -C - -O https://raw.githubusercontent.com/opea-project/GenAIComps/main/comps/third_parties/pathway/src/data/nke-10k-2023.pdf

Now you can use the chatqna-data-prep service to send that document to OpenSearch. Use the following command to map local port 6007 to the services port 6007.

kubectl port-forward -n opensearch svc/chatqna-data-prep 6007:6007 &

Wait until you see the message Forwarding from 127.0.0.1:6007 -> 6007, and then send the document.

![VPC](/images/5.fwd/image089.png)

Data prep will take about 30 seconds processing the document. When it's done you will see

{"status": 200, "message": "Data preparation succeeded"}

To see how OPEA uses OpenSearch, you can query OpenSearch directly. One of OpenSearch's core sets of APIs is the Compact Aligned Text (_cat) API. The _cat API (most OpenSearch APIs begin with an underscore, _) is an administrative API that retrieves information on your cluster, indices, nodes, shards, and more. To see the indices in OpenSearch, execute the following command (if your Cloud Shell has terminated, you'll need to re-establish the port forwarding. See above for instructions)


curl -XGET 'https://localhost:9200/_cat/indices?v' --insecure -u admin:strongOpea0!

You should see output like this:

![VPC](/images/5.fwd/image090.png)

When inspecting the OpenSearch output, you will come across various system indices, often prefixed with a dot. Among them, the audit log provides insights into API usage, while indices like rag-opensearch and file-keys contain data from ChatQnA.

Some indices may appear with a yellow health status, indicating that OpenSearch could not fully allocate a shard. This typically happens when an index has both a primary and a replica shard, but only one node is available in the cluster. Since OpenSearch enforces that primary and replica shards reside on separate nodes, the replica remains unassigned, leading to a yellow status. To ensure high availability and fault tolerance, it is best to configure at least one replica and deploy multiple data nodes.

The storage metrics provide further insights into index structure. The pri.store.size value represents the total on-disk size of primary shards, while store.size includes both primary and replica shards. When only the primary shards are allocated, these values remain identical. The document count reflects how many OpenSearch documents exist within an index. Since the system automatically chunks large documents before indexing, a single documentâ€”such as a Nike financial reportâ€”may be split into multiple indexed segments, with the Nike document in this case comprising 271 chunks.

With data successfully indexed, OpenSearchâ€™s retrieval capabilities can be tested using the retriever microservice. Before executing a query, an embedding must first be generated using the embedding service. For example, to search for Nikeâ€™s revenue in 2023, the query must be transformed into an embedding and stored for processing. Once generated, port forwarding should be established for the chatqna-tei microservice to enable seamless communication with OpenSearch.

kubectl port-forward -n opensearch svc/chatqna-tei 9800:80 &

![VPC](/images/5.fwd/image091.png)

To verify that this call succeeded, you can use echo $question_embedding to see the vector embedding. Now use the following command to call the retriever microservice and find matching documents from OpenSearch and store them in the similar_docs bash variable. Establish port forwarding:


kubectl port-forward -n opensearch svc/chatqna-retriever-usvc 9801:7000 &

After the shell acknowledges that it's forwarding, run the retrieval

![VPC](/images/5.fwd/image092.png)

Again, you can verify the retrieval with echo $similar_docs | jq .. Now you can explore the reranker, contacting it directly with the similar docs and compare with the question What was Nike Revenue in 2023?. The chatqna-teirerank service expects an array of text blocks. Execute the following commands to reformat $similar_docs and save the result in the local file rerank.json

texts=$(echo "$similar_docs" | jq -r '[.retrieved_docs[].text | @json]')
echo "{\"query\":\"What was Nike Revenue in 2023?\", \"texts\": $texts}" | jq -c . > rerank.json

Now establish port forwarding for the chatqna-teirerank service.

kubectl port-forward -n opensearch svc/chatqna-teirerank 9802:80 &

Once the shell responds that it's forwarding, execute the following command to see the rerank results

curl -X POST localhost:9802/rerank \
  -d @rerank.json\
  -H 'Content-Type: application/json'

  You should see output like this. In this case, the top-retrieved item still has the best score after reranking, followed by the third, first, and second.

[{"index":0,"score":0.9984302},{"index":3,"score":0.9972289},{"index":1,"score":0.9776342},{"index":2,"score":0.84730965}]

OPEA will use the first result as context for the chatqna-tgi service. You've now contacted each of the microservices and seen how the data and query are transformed during OPEA's query handling.

As a final test, you can send the query to the load balancer to see the result. Use the following command to get the address of the load balancer:

kubectl get ingress -n opensearch

You should see output like this

opensearch-ingress alb * opensearch-ingress-156457628.us-east-2.elb.amazonaws.com 80 46h

Copy the address of the load balancer and paste it in the below command to see ChatQnA's response for the query "What was the revenue of Nike in 2023?"

curl http://[YOUR INGRESS DNS NAME]/v1/chatqna -H "Content-Type: application/json" -d '{"messages": "What was the revenue of Nike in 2023?"}'

You should see streaming text with the answer: "In fiscal 2023, NIKE, Inc. Revenues were $51.2 billion."

As a final test, copy-paste the ingress URL to your browser, where you can try out the query from the UI.

**Conclusion**

In this task, OpenSearch was deployed as the vector database and its integration with OPEA was tested. Direct connections were made to the data preparation, embedding, and retriever microservices to gain a deeper understanding of how OpenSearch interacts with these components. Additionally, OpenSearchâ€™s query API was explored to examine its document retrieval capabilities. For further exploration, the Explore OpenSearch (Optional) module provides an opportunity to delve deeper into the API and experiment with different query types.